{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç FAISS Vector Search Testing\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a pre-built FAISS index\n",
    "2. Perform semantic search queries\n",
    "3. Understand similarity scores\n",
    "4. Explore search results by category\n",
    "5. Compare different query types\n",
    "\n",
    "**Prerequisites:** Run `04_faiss_index_creation.ipynb` first to generate the FAISS index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install faiss-cpu sentence-transformers pandas numpy matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load FAISS Index and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index\n",
    "print(\"üîÑ Loading FAISS index...\")\n",
    "index = faiss.read_index('sentences.faiss')\n",
    "print(f\"‚úÖ FAISS index loaded! Contains {index.ntotal} vectors\")\n",
    "\n",
    "# Load metadata\n",
    "print(\"\\nüîÑ Loading metadata...\")\n",
    "with open('sentences_metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "sentences = metadata['sentences']\n",
    "categories = metadata['categories']\n",
    "model_name = metadata['model_name']\n",
    "\n",
    "print(f\"‚úÖ Metadata loaded!\")\n",
    "print(f\"üìê Model: {model_name}\")\n",
    "print(f\"üìä Sentences: {len(sentences)}\")\n",
    "print(f\"üìÇ Categories: {list(set(categories))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the same embedding model used for indexing\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"‚úÖ Model '{model_name}' loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Create Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, k: int = 5, show_results: bool = True):\n",
    "    \"\"\"\n",
    "    Search for similar sentences in the FAISS index.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query text\n",
    "        k: Number of results to return\n",
    "        show_results: Whether to print results\n",
    "    \n",
    "    Returns:\n",
    "        List of (sentence, category, score) tuples\n",
    "    \"\"\"\n",
    "    # Generate embedding for query\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if show_results:\n",
    "        print(f\"\\nüîç Query: '{query}'\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        sentence = sentences[idx]\n",
    "        category = categories[idx]\n",
    "        score = float(dist)\n",
    "        results.append((sentence, category, score))\n",
    "        \n",
    "        if show_results:\n",
    "            # Color code based on score\n",
    "            if score >= 0.5:\n",
    "                emoji = \"üü¢\"  # High relevance\n",
    "            elif score >= 0.3:\n",
    "                emoji = \"üü°\"  # Medium relevance\n",
    "            else:\n",
    "                emoji = \"üî¥\"  # Low relevance\n",
    "            \n",
    "            print(f\"\\n{emoji} Rank {i+1} | Score: {score:.4f} | Category: {category}\")\n",
    "            print(f\"   {sentence}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Test Search Queries\n",
    "\n",
    "Let's test different types of queries to understand how semantic search works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Test 1: Technology Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search(\"How do I build AI applications?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üè• Test 2: Health Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search(\"How can I improve my wellbeing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí∞ Test 3: Finance Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search(\"How do I save money for the future?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåç Test 4: Travel Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search(\"What are famous tourist attractions in Asia?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Test 5: Abstract/Conceptual Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search(\"How does the universe work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Interactive Search\n",
    "\n",
    "Try your own queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Enter your own query here!\n",
    "my_query = \"What should I eat for breakfast?\"\n",
    "\n",
    "results = search(my_query, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Category Distribution Analysis\n",
    "\n",
    "Let's analyze which categories appear most frequently in search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_search(query: str, k: int = 20):\n",
    "    \"\"\"\n",
    "    Analyze the category distribution of search results.\n",
    "    \"\"\"\n",
    "    results = search(query, k=k, show_results=False)\n",
    "    \n",
    "    # Count categories\n",
    "    category_counts = {}\n",
    "    category_scores = {}\n",
    "    \n",
    "    for sentence, category, score in results:\n",
    "        category_counts[category] = category_counts.get(category, 0) + 1\n",
    "        if category not in category_scores:\n",
    "            category_scores[category] = []\n",
    "        category_scores[category].append(score)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_scores = {cat: np.mean(scores) for cat, scores in category_scores.items()}\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Category counts\n",
    "    categories_sorted = sorted(category_counts.keys(), key=lambda x: category_counts[x], reverse=True)\n",
    "    counts = [category_counts[c] for c in categories_sorted]\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(categories_sorted)))\n",
    "    axes[0].barh(categories_sorted, counts, color=colors)\n",
    "    axes[0].set_xlabel('Count')\n",
    "    axes[0].set_title(f'Category Distribution (Top {k} results)')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Plot 2: Average scores\n",
    "    scores = [avg_scores[c] for c in categories_sorted]\n",
    "    axes[1].barh(categories_sorted, scores, color=colors)\n",
    "    axes[1].set_xlabel('Average Similarity Score')\n",
    "    axes[1].set_title('Average Score by Category')\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    \n",
    "    plt.suptitle(f\"Query: '{query}'\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return category_counts, avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different queries\n",
    "counts, scores = analyze_search(\"How do computers learn from data?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, scores = analyze_search(\"I want to visit beautiful places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, scores = analyze_search(\"Tell me about wild animals in nature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Score Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_score_distribution(query: str):\n",
    "    \"\"\"\n",
    "    Visualize the similarity score distribution across all documents.\n",
    "    \"\"\"\n",
    "    # Get all results\n",
    "    results = search(query, k=len(sentences), show_results=False)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results, columns=['sentence', 'category', 'score'])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Score histogram\n",
    "    axes[0].hist(df['score'], bins=30, edgecolor='white', color='steelblue', alpha=0.7)\n",
    "    axes[0].axvline(x=df['score'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"score\"].mean():.3f}')\n",
    "    axes[0].axvline(x=df['score'].median(), color='orange', linestyle='--', label=f'Median: {df[\"score\"].median():.3f}')\n",
    "    axes[0].set_xlabel('Similarity Score')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Score Distribution')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot 2: Box plot by category\n",
    "    df_sorted = df.sort_values('score', ascending=False)\n",
    "    category_order = df.groupby('category')['score'].mean().sort_values(ascending=False).index\n",
    "    \n",
    "    palette = sns.color_palette(\"viridis\", len(category_order))\n",
    "    sns.boxplot(data=df, x='score', y='category', order=category_order, palette=palette, ax=axes[1])\n",
    "    axes[1].set_xlabel('Similarity Score')\n",
    "    axes[1].set_ylabel('Category')\n",
    "    axes[1].set_title('Score Distribution by Category')\n",
    "    \n",
    "    plt.suptitle(f\"Query: '{query}'\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top categories\n",
    "    print(\"\\nüìä Category Rankings (by average score):\")\n",
    "    for i, cat in enumerate(category_order[:5]):\n",
    "        avg = df[df['category'] == cat]['score'].mean()\n",
    "        print(f\"  {i+1}. {cat}: {avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_score_distribution(\"What is the best exercise for staying fit?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_score_distribution(\"Coffee and food recipes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Compare Similar Queries\n",
    "\n",
    "See how different phrasings of the same question affect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_queries(queries: list, k: int = 5):\n",
    "    \"\"\"\n",
    "    Compare search results for multiple queries.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        results = search(query, k=k, show_results=False)\n",
    "        all_results[query] = results\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä QUERY COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nüîç Query: '{query}'\")\n",
    "        print(\"-\"*60)\n",
    "        for i, (sentence, category, score) in enumerate(all_results[query]):\n",
    "            print(f\"  {i+1}. [{category:10}] (Score: {score:.3f}) {sentence[:60]}...\")\n",
    "    \n",
    "    # Find common results\n",
    "    sets = [set(r[0] for r in results) for results in all_results.values()]\n",
    "    common = sets[0].intersection(*sets[1:])\n",
    "    \n",
    "    if common:\n",
    "        print(f\"\\n‚ú® Common results across all queries ({len(common)}):\")\n",
    "        for sentence in common:\n",
    "            print(f\"  ‚Ä¢ {sentence[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare similar queries about AI/ML\n",
    "compare_queries([\n",
    "    \"How does AI work?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me about artificial intelligence and neural networks\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare similar queries about money\n",
    "compare_queries([\n",
    "    \"How to become rich?\",\n",
    "    \"Investment strategies for beginners\",\n",
    "    \"Financial planning and savings\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Threshold-Based Filtering\n",
    "\n",
    "In production, you often want to filter results by a minimum similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_threshold(query: str, threshold: float = 0.3, max_results: int = 10):\n",
    "    \"\"\"\n",
    "    Search with a minimum similarity threshold.\n",
    "    \"\"\"\n",
    "    results = search(query, k=max_results, show_results=False)\n",
    "    filtered = [(s, c, score) for s, c, score in results if score >= threshold]\n",
    "    \n",
    "    print(f\"\\nüîç Query: '{query}'\")\n",
    "    print(f\"üìè Threshold: {threshold}\")\n",
    "    print(f\"üìä Results: {len(filtered)} / {len(results)} passed threshold\\n\")\n",
    "    \n",
    "    if filtered:\n",
    "        for i, (sentence, category, score) in enumerate(filtered):\n",
    "            print(f\"  {i+1}. [Score: {score:.4f}] [{category}]\")\n",
    "            print(f\"     {sentence}\\n\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No results above threshold!\")\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different thresholds\n",
    "results = search_with_threshold(\"Python programming language\", threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher threshold = fewer but more relevant results\n",
    "results = search_with_threshold(\"Python programming language\", threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an unrelated query\n",
    "results = search_with_threshold(\"asdfghjkl random text\", threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "\n",
    "1. ‚úÖ **Loading FAISS Index** - Fast loading of pre-built vector database\n",
    "2. ‚úÖ **Semantic Search** - Finding similar sentences based on meaning, not keywords\n",
    "3. ‚úÖ **Score Interpretation** - Understanding similarity scores (0-1 range)\n",
    "4. ‚úÖ **Category Analysis** - Visualizing which categories match queries\n",
    "5. ‚úÖ **Query Comparison** - How different phrasings affect results\n",
    "6. ‚úÖ **Threshold Filtering** - Production-ready result filtering\n",
    "\n",
    "## üí° Key Insights\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Semantic Search** | Finds results by meaning, not exact keywords |\n",
    "| **Similarity Score** | Higher = more similar (range 0-1 for cosine similarity) |\n",
    "| **Thresholds** | Use 0.3-0.5 for balanced precision/recall |\n",
    "| **Query Phrasing** | Similar questions can get slightly different results |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps for RAG Systems\n",
    "\n",
    "To build a complete RAG system, you would:\n",
    "\n",
    "1. **Retrieve** - Use FAISS to find top-K relevant documents\n",
    "2. **Augment** - Add retrieved documents to your LLM prompt\n",
    "3. **Generate** - Use an LLM (GPT, Gemini, etc.) to generate a response\n",
    "\n",
    "```python\n",
    "# Example RAG pseudocode\n",
    "user_query = \"How does machine learning work?\"\n",
    "\n",
    "# Step 1: Retrieve\n",
    "results = search(user_query, k=3)\n",
    "context = \"\\n\".join([r[0] for r in results])\n",
    "\n",
    "# Step 2: Augment\n",
    "prompt = f\"\"\"\n",
    "Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Generate\n",
    "response = llm.generate(prompt)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
