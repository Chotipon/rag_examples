{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Part 2: Multi-Model Comparison\n",
    "\n",
    "Different embedding models produce different vector representations. In this notebook, we'll compare how various models represent the **same words** differently.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. How different models produce different embeddings\n",
    "2. Why model choice matters for RAG systems\n",
    "3. Trade-offs between model size, speed, and quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this in Colab)\n",
    "!pip install sentence-transformers plotly seaborn scikit-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Define Your Word List\n",
    "\n",
    "We'll use the same word list across all models to compare their representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üîß CUSTOMIZE YOUR WORD LIST HERE!\n",
    "# ============================================================\n",
    "# Use the same categories as Notebook 1 for consistency\n",
    "\n",
    "word_categories = {\n",
    "    \"üçé Fruits\": [\n",
    "        \"apple\", \"banana\", \"orange\", \"mango\", \"strawberry\", \"grape\"\n",
    "    ],\n",
    "    \"üêæ Animals\": [\n",
    "        \"dog\", \"cat\", \"elephant\", \"lion\", \"tiger\", \"rabbit\"\n",
    "    ],\n",
    "    \"üé® Colors\": [\n",
    "        \"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\"\n",
    "    ],\n",
    "    \"üíª Technology\": [\n",
    "        \"computer\", \"smartphone\", \"laptop\", \"tablet\", \"keyboard\", \"mouse\"\n",
    "    ],\n",
    "    \"üöó Vehicles\": [\n",
    "        \"car\", \"bicycle\", \"motorcycle\", \"airplane\", \"train\", \"boat\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten the dictionary\n",
    "words = []\n",
    "categories = []\n",
    "for category, word_list in word_categories.items():\n",
    "    words.extend(word_list)\n",
    "    categories.extend([category] * len(word_list))\n",
    "\n",
    "print(f\"üìä Total words: {len(words)}\")\n",
    "print(f\"üìÅ Categories: {list(word_categories.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ Define Models to Compare\n",
    "\n",
    "Here are some popular embedding models with different characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üîß ADD OR REMOVE MODELS HERE!\n",
    "# ============================================================\n",
    "\n",
    "models_to_compare = {\n",
    "    \"all-MiniLM-L6-v2\": {\n",
    "        \"description\": \"Fast & lightweight (22M params, 384 dim)\",\n",
    "        \"size\": \"Small\",\n",
    "        \"speed\": \"‚ö° Fast\"\n",
    "    },\n",
    "    \"all-mpnet-base-v2\": {\n",
    "        \"description\": \"Higher quality (110M params, 768 dim)\",\n",
    "        \"size\": \"Medium\",\n",
    "        \"speed\": \"üîÑ Moderate\"\n",
    "    },\n",
    "    \"paraphrase-MiniLM-L6-v2\": {\n",
    "        \"description\": \"Optimized for paraphrase detection (22M params, 384 dim)\",\n",
    "        \"size\": \"Small\",\n",
    "        \"speed\": \"‚ö° Fast\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Models to compare:\")\n",
    "print(\"=\" * 70)\n",
    "for name, info in models_to_compare.items():\n",
    "    print(f\"  ‚Ä¢ {name}\")\n",
    "    print(f\"    {info['description']}\")\n",
    "    print(f\"    Size: {info['size']} | Speed: {info['speed']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Load Models & Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings and models\n",
    "model_embeddings = {}\n",
    "loaded_models = {}\n",
    "\n",
    "for model_name in models_to_compare.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîÑ Loading: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    loaded_models[model_name] = model\n",
    "    \n",
    "    print(f\"   Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(f\"   Generating embeddings...\")\n",
    "    embeddings = model.encode(words, show_progress_bar=True)\n",
    "    model_embeddings[model_name] = embeddings\n",
    "    \n",
    "    print(f\"   ‚úÖ Done! Shape: {embeddings.shape}\")\n",
    "\n",
    "print(f\"\\n\\nüéâ All {len(models_to_compare)} models loaded and embeddings generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üåê Side-by-Side 3D Visualizations\n",
    "\n",
    "Let's see how each model represents the same words in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE to each model's embeddings\n",
    "tsne_results = {}\n",
    "\n",
    "perplexity = min(30, len(words) - 1)\n",
    "\n",
    "for model_name, embeddings in model_embeddings.items():\n",
    "    print(f\"üîÑ Applying t-SNE for {model_name}...\")\n",
    "    \n",
    "    tsne = TSNE(\n",
    "        n_components=3,\n",
    "        perplexity=perplexity,\n",
    "        random_state=42,\n",
    "        n_iter=1000,\n",
    "        learning_rate='auto',\n",
    "        init='pca'\n",
    "    )\n",
    "    \n",
    "    embeddings_3d = tsne.fit_transform(embeddings)\n",
    "    tsne_results[model_name] = embeddings_3d\n",
    "    print(f\"   ‚úÖ Done!\")\n",
    "\n",
    "print(\"\\nüéâ All t-SNE transformations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual 3D plots for each model\n",
    "for model_name, embeddings_3d in tsne_results.items():\n",
    "    df = pd.DataFrame({\n",
    "        'word': words,\n",
    "        'category': categories,\n",
    "        'x': embeddings_3d[:, 0],\n",
    "        'y': embeddings_3d[:, 1],\n",
    "        'z': embeddings_3d[:, 2]\n",
    "    })\n",
    "    \n",
    "    fig = px.scatter_3d(\n",
    "        df,\n",
    "        x='x', y='y', z='z',\n",
    "        color='category',\n",
    "        text='word',\n",
    "        title=f'ü§ñ {model_name}<br><sub>{models_to_compare[model_name][\"description\"]}</sub>',\n",
    "        labels={'x': 't-SNE 1', 'y': 't-SNE 2', 'z': 't-SNE 3'},\n",
    "        height=600,\n",
    "        color_discrete_sequence=px.colors.qualitative.Set1\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(\n",
    "        marker=dict(size=8, line=dict(width=1, color='white')),\n",
    "        textposition='top center',\n",
    "        textfont=dict(size=9)\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.15,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=100, t=80)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    print(f\"\\n{'‚îÄ'*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Compare Similarity Matrices\n",
    "\n",
    "Let's see how each model measures word similarity differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrices for each model\n",
    "similarity_matrices = {}\n",
    "\n",
    "for model_name, embeddings in model_embeddings.items():\n",
    "    similarity_matrices[model_name] = cosine_similarity(embeddings)\n",
    "    \n",
    "print(\"‚úÖ Similarity matrices calculated for all models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side heatmaps\n",
    "n_models = len(models_to_compare)\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(7 * n_models, 6))\n",
    "\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (model_name, sim_matrix) in zip(axes, similarity_matrices.items()):\n",
    "    sns.heatmap(\n",
    "        sim_matrix,\n",
    "        xticklabels=words,\n",
    "        yticklabels=words,\n",
    "        cmap='RdYlBu_r',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        ax=ax,\n",
    "        cbar_kws={'shrink': 0.8}\n",
    "    )\n",
    "    ax.set_title(f'{model_name}\\n{models_to_compare[model_name][\"size\"]}', fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=90, labelsize=7)\n",
    "    ax.tick_params(axis='y', rotation=0, labelsize=7)\n",
    "\n",
    "plt.suptitle('Cosine Similarity Comparison Across Models', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Compare Specific Word Pairs Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define interesting word pairs to compare\n",
    "word_pairs_to_compare = [\n",
    "    (\"dog\", \"cat\"),           # Similar animals\n",
    "    (\"apple\", \"banana\"),       # Similar fruits\n",
    "    (\"car\", \"bicycle\"),        # Similar vehicles\n",
    "    (\"computer\", \"laptop\"),    # Very similar tech\n",
    "    (\"dog\", \"car\"),            # Different categories\n",
    "    (\"apple\", \"red\"),          # Different but related\n",
    "    (\"orange\", \"orange\"),      # Same word (should be 1.0)\n",
    "    (\"mouse\", \"cat\"),          # Ambiguous - computer mouse vs animal\n",
    "]\n",
    "\n",
    "print(\"üìä SIMILARITY COMPARISON ACROSS MODELS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Word Pair':<25}\", end=\"\")\n",
    "for model_name in models_to_compare.keys():\n",
    "    short_name = model_name.split('-')[1] if '-' in model_name else model_name[:10]\n",
    "    print(f\"{short_name:^15}\", end=\"\")\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "for word1, word2 in word_pairs_to_compare:\n",
    "    if word1 in words and word2 in words:\n",
    "        idx1 = words.index(word1)\n",
    "        idx2 = words.index(word2)\n",
    "        \n",
    "        print(f\"{word1} ‚Üî {word2:<15}\", end=\"\")\n",
    "        \n",
    "        for model_name, sim_matrix in similarity_matrices.items():\n",
    "            similarity = sim_matrix[idx1, idx2]\n",
    "            # Color code the similarity\n",
    "            if similarity > 0.7:\n",
    "                indicator = \"üü¢\"\n",
    "            elif similarity > 0.4:\n",
    "                indicator = \"üü°\"\n",
    "            else:\n",
    "                indicator = \"üî¥\"\n",
    "            print(f\"{indicator} {similarity:.3f}      \", end=\"\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"{word1} ‚Üî {word2}: ‚ö†Ô∏è Word not in list\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Legend: üü¢ High (>0.7) | üü° Medium (0.4-0.7) | üî¥ Low (<0.4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Similarity Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution of similarities across models\n",
    "fig, axes = plt.subplots(1, len(models_to_compare), figsize=(5 * len(models_to_compare), 4))\n",
    "\n",
    "if len(models_to_compare) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (model_name, sim_matrix) in zip(axes, similarity_matrices.items()):\n",
    "    # Get upper triangle values (excluding diagonal)\n",
    "    upper_tri = sim_matrix[np.triu_indices(len(words), k=1)]\n",
    "    \n",
    "    ax.hist(upper_tri, bins=30, edgecolor='white', alpha=0.7, color='steelblue')\n",
    "    ax.axvline(np.mean(upper_tri), color='red', linestyle='--', label=f'Mean: {np.mean(upper_tri):.3f}')\n",
    "    ax.axvline(np.median(upper_tri), color='orange', linestyle='--', label=f'Median: {np.median(upper_tri):.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Cosine Similarity')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{model_name}\\n(Std: {np.std(upper_tri):.3f})', fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "plt.suptitle('Distribution of Pairwise Similarities', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üèÜ Model Ranking: Which Clusters Best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate intra-category vs inter-category similarity\n",
    "print(\"üìä CLUSTERING QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Measuring how well each model separates categories...\")\n",
    "print()\n",
    "\n",
    "model_scores = {}\n",
    "\n",
    "for model_name, sim_matrix in similarity_matrices.items():\n",
    "    intra_similarities = []  # Same category\n",
    "    inter_similarities = []  # Different category\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        for j in range(i + 1, len(words)):\n",
    "            if categories[i] == categories[j]:\n",
    "                intra_similarities.append(sim_matrix[i, j])\n",
    "            else:\n",
    "                inter_similarities.append(sim_matrix[i, j])\n",
    "    \n",
    "    avg_intra = np.mean(intra_similarities)\n",
    "    avg_inter = np.mean(inter_similarities)\n",
    "    separation = avg_intra - avg_inter  # Higher = better separation\n",
    "    \n",
    "    model_scores[model_name] = {\n",
    "        'intra': avg_intra,\n",
    "        'inter': avg_inter,\n",
    "        'separation': separation\n",
    "    }\n",
    "    \n",
    "    print(f\"ü§ñ {model_name}\")\n",
    "    print(f\"   Avg similarity (same category):      {avg_intra:.4f}\")\n",
    "    print(f\"   Avg similarity (different category): {avg_inter:.4f}\")\n",
    "    print(f\"   Category separation score:           {separation:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Determine winner\n",
    "best_model = max(model_scores.keys(), key=lambda x: model_scores[x]['separation'])\n",
    "print(\"=\" * 80)\n",
    "print(f\"üèÜ Best clustering model: {best_model}\")\n",
    "print(f\"   (Highest separation between same-category and different-category pairs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### Model Differences\n",
    "- **Embedding dimension** affects the richness of representations\n",
    "- **Training objective** (e.g., paraphrase vs semantic similarity) affects results\n",
    "- **Model size** trades off speed vs quality\n",
    "\n",
    "### Choosing a Model for RAG\n",
    "1. **For speed**: Use `all-MiniLM-L6-v2` or similar lightweight models\n",
    "2. **For quality**: Use `all-mpnet-base-v2` or larger models\n",
    "3. **For multilingual**: Use `paraphrase-multilingual-*` models\n",
    "4. **For domain-specific**: Consider fine-tuning or specialized models\n",
    "\n",
    "### What We Learned\n",
    "- Different models produce different similarity scores for the same words\n",
    "- Some models cluster categories more clearly than others\n",
    "- The \"best\" model depends on your specific use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Try It Yourself!\n",
    "\n",
    "Experiment with:\n",
    "1. Adding more models to `models_to_compare`\n",
    "2. Changing the word categories\n",
    "3. Testing with longer phrases instead of single words\n",
    "4. Comparing multilingual models with different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß YOUR EXPERIMENTATION SPACE\n",
    "# Add your own tests below!\n",
    "\n",
    "# Example: Test a custom phrase\n",
    "custom_phrases = [\n",
    "    \"I love eating apples\",\n",
    "    \"Apples are my favorite fruit\",\n",
    "    \"The dog is playing in the park\"\n",
    "]\n",
    "\n",
    "print(\"Comparing custom phrases across models:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, model in loaded_models.items():\n",
    "    embeddings = model.encode(custom_phrases)\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    print(f\"\\nü§ñ {model_name}:\")\n",
    "    print(f\"   Phrase 1 ‚Üî Phrase 2: {sim_matrix[0,1]:.4f}\")\n",
    "    print(f\"   Phrase 1 ‚Üî Phrase 3: {sim_matrix[0,2]:.4f}\")\n",
    "    print(f\"   Phrase 2 ‚Üî Phrase 3: {sim_matrix[1,2]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
